[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Eduardo Lima‚Äôs Blog",
    "section": "",
    "text": "Annotation Error Detection in Legal NER Datasets with Cleanlab\n\n\n\nData Science\n\nAnnotation Error Detection\n\nNamed Entity Recognition\n\n\n\nThis post applies confident learning techniques with Cleanlab to detect annotation errors in the LeNER-Br dataset, enhancing the quality of legal named entity recognition tasks.\n\n\n\n\n\nMay 18, 2025\n\n\n\n\n\n\n\nUsing LLMs to Extract Legal Entities from Audit Court Decisions\n\n\n\nData Science\n\nLLM\n\n\n\nThis post demonstrates a hybrid pipeline with regular expressions and LLMs to extract fines, reimbursements, and recommendations from Brazilian audit court decisions.\n\n\n\n\n\nJan 22, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/annotation-error-detection-legal-ner-cleanlab.html",
    "href": "posts/annotation-error-detection-legal-ner-cleanlab.html",
    "title": "Annotation Error Detection in Legal NER Datasets with Cleanlab",
    "section": "",
    "text": "Annotation Error Detection in the LeNER-Br Dataset\nAnnotation Error Detection is a technique used to identify inconsistencies or incorrect labels in manually annotated datasets. These errors can compromise the quality of models trained on this data for tasks such as Named Entity Recognition. Detection tools and methods aim to locate these flaws automatically, ensuring greater data reliability and better model performance.\nIn this notebook, we will analyze the LeNER-Br dataset with the goal of identifying possible annotation errors using the confident learning technique, implemented by the Cleanlab library. This approach allows for the detection of incorrectly labeled instances based on the probabilistic predictions of a classifier, as we will see in the code below.\nThe LeNER-Br dataset is a Portuguese corpus focused on Named Entity Recognition (NER) in Brazilian legal texts. Developed by Luz et al.¬†(2018), LeNER-Br is composed exclusively of legal documents, such as judicial decisions and opinions, collected from various Brazilian courts. It was manually annotated to identify entities like people, organizations, locations, and temporal expressions, in addition to specific legal categories such as LEGISLATION and JURISPRUDENCE, which are not common in other Portuguese corpora. The complete description of the work can be read in the article available at https://teodecampos.github.io/LeNER-Br/luz_etal_propor2018.pdf\n\n\nEnvironment Setup\nWe install the Cleanlab library, which will be used to apply confident learning techniques to identify possible annotation errors in the LeNER-Br dataset. Then, we import the necessary libraries for the rest of the analysis and download the training and test files directly from the official LeNER-Br repository. As the files are in CoNLL format, which organizes data in columns, it is necessary to convert them to the BIO (Beginning, Inside, Outside) format, widely used in Named Entity Recognition tasks, to facilitate subsequent processing.\n#!pip install cleanlab\nimport os\nimport re\nimport requests\nimport zipfile\nimport numpy as np\nimport pandas as pd\nimport torch\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import SGDClassifier\n\nfrom transformers import AutoTokenizer, AutoModel\nfrom cleanlab.filter import find_label_issues\n!wget https://raw.githubusercontent.com/eduardoplima/aed-lener-br/refs/heads/main/leNER-Br/train/train.conll\n!wget https://raw.githubusercontent.com/eduardoplima/aed-lener-br/refs/heads/main/leNER-Br/test/test.conll\n--2025-05-17 21:16:22--  https://raw.githubusercontent.com/eduardoplima/aed-lener-br/refs/heads/main/leNER-Br/train/train.conll\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2142199 (2.0M) [text/plain]\nSaving to: ‚Äòtrain.conll‚Äô\n\ntrain.conll         100%[===================&gt;]   2.04M  --.-KB/s    in 0.01s   \n\n2025-05-17 21:16:22 (198 MB/s) - ‚Äòtrain.conll‚Äô saved [2142199/2142199]\n\n--2025-05-17 21:16:22--  https://raw.githubusercontent.com/eduardoplima/aed-lener-br/refs/heads/main/leNER-Br/test/test.conll\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 438441 (428K) [text/plain]\nSaving to: ‚Äòtest.conll‚Äô\n\ntest.conll          100%[===================&gt;] 428.17K  --.-KB/s    in 0.004s  \n\n2025-05-17 21:16:23 (106 MB/s) - ‚Äòtest.conll‚Äô saved [438441/438441]\nNUM_FOLDS_CV = 5\nRANDOM_SEED = 43 # almost 42 üòÇ (and prime!)\ndef load_conll_lener(file_path):\n    sentences = []\n    current_sentence_tokens = []\n    current_sentence_labels = []\n\n    with open(file_path, 'r', encoding='utf-8') as f:\n        for line in f:\n          line = line.strip()\n          if line:\n              parts = line.split()\n              if len(parts) &gt;= 2:\n                  token = parts[0]\n                  ner_tag = parts[-1]\n                  current_sentence_tokens.append(token)\n                  current_sentence_labels.append(ner_tag)\n              else:\n                  pass # Or handle malformed lines\n          else:\n              if current_sentence_tokens:\n                  sentences.append(list(zip(current_sentence_tokens, current_sentence_labels)))\n                  current_sentence_tokens = []\n                  current_sentence_labels = []\n\n        if current_sentence_tokens: # Add last sentence if file doesn't end with a blank line\n            sentences.append(list(zip(current_sentence_tokens, current_sentence_labels)))\n\n    return sentences\ntraining_sentences = load_conll_lener(\"train.conll\")\nprint(f\"\\nLoaded {len(training_sentences)} sentences from train.conll.\")\nLoaded 7827 sentences from train.conll.\nBelow are some example sentences in BIO format.\nprint(\"\\nExample sentence:\")\nif len(training_sentences) &gt; 5:\n    for token, label in training_sentences[5][-50:]:\n        print(f\"{token}\\t{label}\")\nExample sentence:\nV.v O\nAPELA√á√ÉO    O\nC√çVEL   O\n-   O\nNULIDADE    O\nPROCESSUAL  O\n-   O\nINTIMA√á√ÉO   O\nDO  O\nMINIST√âRIO  B-ORGANIZACAO\nP√öBLICO I-ORGANIZACAO\n-   O\nINCAPAZ O\nACOMPANHADA O\nDE  O\nREPRESENTANTE   O\nLEGAL   O\nE   O\nDE  O\nADVOGADO    O\n-   O\nEXERC√çCIO   O\nDO  O\nCONTRADIT√ìRIO   O\nE   O\nDA  O\nAMPLA   O\nDEFESA  O\n-   O\nAUS√äNCIA    O\nDE  O\nPREJU√çZOS   O\n-   O\nV√çCIO   O\nAFASTADO    O\n-   O\nIMPROCED√äNCIA   O\nDO  O\nPEDIDO  O\n-   O\nINEXIST√äNCIA    O\nDE  O\nPROVA   O\nQUANTO  O\nAO  O\nFATO    O\nCONSTITUTIVO    O\nDO  O\nDIREITO O\n.   O\n\n\nToken Extraction\nAs we saw, the extracted sentences are organized into tuples of tokens and their respective BIO labels. The goal is to extract features for each token from our training sentences using a BERT model via Hugging Face: in this case, neuralmind/bert-large-portuguese-cased. We explicitly define a maximum size of 512, the standard size for the chosen BERT model, to avoid OverflowError.\nThe process followed for each sentence is as follows: first, we extract the token texts. Then, we use tokenizer_hf to convert these texts into a format that the BERT model understands, specifying that the input is already divided into words (is_split_into_words=True) and moving the data to the processing device (CPU or GPU). With the inputs ready, we pass them through model_hf to obtain the ‚Äúhidden states‚Äù of the last layer, which are the contextual embeddings for each subword generated by the tokenizer. Since BERT works with subwords, we need to align these embeddings back to our original tokens. For this, we use the word_ids provided by the tokenizer and, for each original token, we calculate the average of the embeddings of its constituent subwords. These mean vectors are then added to our final features_tokens list, numerically representing each word in our corpus.\nhf_model_name = \"neuralmind/bert-large-portuguese-cased\"\n\ntokenizer_hf = AutoTokenizer.from_pretrained(hf_model_name)\nmodel_hf = AutoModel.from_pretrained(hf_model_name)\ntokenizer_config.json:   0%|          | 0.00/155 [00:00&lt;?, ?B/s]\n\n\n\nconfig.json:   0%|          | 0.00/648 [00:00&lt;?, ?B/s]\n\n\n\nvocab.txt:   0%|          | 0.00/210k [00:00&lt;?, ?B/s]\n\n\n\nadded_tokens.json:   0%|          | 0.00/2.00 [00:00&lt;?, ?B/s]\n\n\n\nspecial_tokens_map.json:   0%|          | 0.00/112 [00:00&lt;?, ?B/s]\n\n\n\npytorch_model.bin:   0%|          | 0.00/1.34G [00:00&lt;?, ?B/s]\nall_tokens = []\nall_labels = []\nids_sentences= []\n\nfor i, sentence in enumerate(training_sentences):\n  for token_text, ner_tag in sentence:\n    all_tokens.append(token_text)\n    all_labels.append(ner_tag)\n    ids_sentences.append(i)\n\nprint(f\"\\nTotal tokens in the training data: {len(all_tokens)}\")\nprint(f\"Unique LeNER-Br labels: {sorted(list(set(all_labels)))}\")\nTotal tokens in the training data: 229277\nUnique LeNER-Br labels: ['B-JURISPRUDENCIA', 'B-LEGISLACAO', 'B-LOCAL', 'B-ORGANIZACAO', 'B-PESSOA', 'B-TEMPO', 'I-JURISPRUDENCIA', 'I-LEGISLACAO', 'I-LOCAL', 'I-ORGANIZACAO', 'I-PESSOA', 'I-TEMPO', 'O']\nWe have a total of 13 unique labels in our dataset. Below is an explanation of the meaning of each one:\n\nB-JURISPRUDENCIA:\n\nB: Indicates that this token is the beginning of a named entity.\nJURISPRUDENCIA: Indicates that the named entity is of type ‚ÄúJurisprudence‚Äù. Refers to judicial decisions, rulings, precedents, or any set of interpretations of laws made by courts.\nExample: In the text ‚ÄúConforme o Ac√≥rd√£o n¬∫ 123‚Ä¶‚Äù (According to Ruling No.¬†123‚Ä¶), ‚ÄúAc√≥rd√£o‚Äù could be B-JURISPRUDENCIA.\n\nB-LEGISLACAO:\n\nB: Beginning of the entity.\nLEGISLACAO: Indicates that the named entity is of type ‚ÄúLegislation‚Äù. Refers to laws, decrees, ordinances, codes, constitutions, etc.\nExample: In the text ‚ÄúA Lei n¬∫ 8.666/93‚Ä¶‚Äù (Law No.¬†8.666/93‚Ä¶), ‚ÄúLei‚Äù could be B-LEGISLACAO.\n\nB-LOCAL:\n\nB: Beginning of the entity.\nLOCAL: Indicates that the named entity is a ‚ÄúLocation‚Äù. It can be a city, state, country, address, geographical feature, etc.\nExample: In the text ‚ÄúEle viajou para Paris‚Ä¶‚Äù (He traveled to Paris‚Ä¶), ‚ÄúParis‚Äù would be B-LOCAL.\n\nB-ORGANIZACAO:\n\nB: Beginning of the entity.\nORGANIZACAO: Indicates that the named entity is an ‚ÄúOrganization‚Äù. Includes companies, government institutions, NGOs, sports teams, etc.\nExample: In the text ‚ÄúO Google anunciou‚Ä¶‚Äù (Google announced‚Ä¶), ‚ÄúGoogle‚Äù would be B-ORGANIZACAO.\n\nB-PESSOA:\n\nB: Beginning of the entity.\nPESSOA: Indicates that the named entity is a ‚ÄúPerson‚Äù. Refers to names of individuals.\nExample: In the text ‚ÄúMaria Silva √© advogada‚Ä¶‚Äù (Maria Silva is a lawyer‚Ä¶), ‚ÄúMaria‚Äù would be B-PESSOA.\n\nB-TEMPO:\n\nB: Beginning of the entity.\nTEMPO: Indicates that the named entity is a temporal reference. It can be a date, time, specific period (e.g., ‚Äú21st century‚Äù, ‚Äúnext week‚Äù).\nExample: In the text ‚ÄúA reuni√£o ser√° em 15 de maio‚Ä¶‚Äù (The meeting will be on May 15th‚Ä¶), ‚Äú15‚Äù could be B-TEMPO.\n\nI-JURISPRUDENCIA:\n\nI: Indicates that this token is inside a ‚ÄúJurisprudence‚Äù type entity that has already begun. It is a continuation of the entity.\nExample: In the text ‚Äú‚Ä¶o Superior Tribunal de Justi√ßa‚Ä¶‚Äù (‚Ä¶the Superior Court of Justice‚Ä¶), if ‚ÄúSuperior‚Äù was B-JURISPRUDENCIA (or B-ORGANIZACAO depending on the scheme), ‚ÄúTribunal‚Äù could be I-JURISPRUDENCIA (or I-ORGANIZACAO). In the case of a long jurisprudence name, like ‚ÄúS√∫mula Vinculante n¬∫ 56‚Äù (Binding Precedent No.¬†56), ‚ÄúVinculante‚Äù, ‚Äún¬∫‚Äù, and ‚Äú56‚Äù would be I-JURISPRUDENCIA if ‚ÄúS√∫mula‚Äù was B-JURISPRUDENCIA.\n\nI-LEGISLACAO:\n\nI: Inside a ‚ÄúLegislation‚Äù type entity.\nExample: In the text ‚ÄúA Lei de Licita√ß√µes‚Ä¶‚Äù (The Bidding Law‚Ä¶), if ‚ÄúLei‚Äù was B-LEGISLACAO, ‚Äúde‚Äù and ‚ÄúLicita√ß√µes‚Äù would be I-LEGISLACAO.\n\nI-LOCAL:\n\nI: Inside a ‚ÄúLocation‚Äù type entity.\nExample: In the text ‚ÄúEle mora em Nova York‚Ä¶‚Äù (He lives in New York‚Ä¶), if ‚ÄúNova‚Äù was B-LOCAL, ‚ÄúYork‚Äù would be I-LOCAL.\n\nI-ORGANIZACAO:\n\nI: Inside an ‚ÄúOrganization‚Äù type entity.\nExample: In the text ‚ÄúO Banco Central do Brasil‚Ä¶‚Äù (The Central Bank of Brazil‚Ä¶), if ‚ÄúBanco‚Äù was B-ORGANIZACAO, ‚ÄúCentral‚Äù would be I-ORGANIZACAO.\n\nI-PESSOA:\n\nI: Inside a ‚ÄúPerson‚Äù type entity.\nExample: In the text ‚ÄúMaria Joaquina da Silva‚Ä¶‚Äù, if ‚ÄúMaria‚Äù was B-PESSOA, ‚ÄúJoaquina‚Äù would be I-PESSOA.\n\nI-TEMPO:\n\nI: Inside a ‚ÄúTime‚Äù type entity.\nExample: In the text ‚ÄúA reuni√£o ser√° em 15 de maio de 2025‚Ä¶‚Äù (The meeting will be on May 15, 2025‚Ä¶), if ‚Äú15‚Äù was B-TEMPO, ‚Äúde‚Äù, ‚Äúmaio‚Äù, ‚Äúde‚Äù, and ‚Äú2025‚Äù would be I-TEMPO.\n\nO:\n\nO: Indicates that the token is outside any named entity. It is a common token that is not part of a specific category of interest.\nExample: In the text ‚ÄúO gato sentou no tapete.‚Äù (The cat sat on the carpet.), ‚Äúsentou‚Äù would be O.\n\n\nimport numpy as np\nimport torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nEFFECTIVE_MAX_LENGTH = 512\n\nmodel_hf.to(device)\nmodel_hf.eval()\n\nfeatures_tokens = []\n\nfor i_sent, sentence_data in enumerate(training_sentences):\n    sentence_texts = [token_data[0] for token_data in sentence_data]\n\n    if not sentence_texts:\n        continue\n\n    inputs = tokenizer_hf(\n        sentence_texts,\n        is_split_into_words=True,\n        return_tensors=\"pt\",\n        padding=\"longest\",\n        truncation=True,\n        max_length=EFFECTIVE_MAX_LENGTH\n    ).to(device)\n\n    word_ids = inputs.word_ids()\n    with torch.no_grad():\n        outputs = model_hf(**inputs)\n        last_hidden_state = outputs.last_hidden_state\n\n    token_subword_embeddings = [[] for _ in range(len(sentence_texts))]\n\n    for subword_idx, original_word_idx in enumerate(word_ids):\n        if original_word_idx is not None:\n            embedding = last_hidden_state[0, subword_idx, :]\n            token_subword_embeddings[original_word_idx].append(embedding)\n\n    current_sentence_token_features = []\n    for original_token_idx in range(len(sentence_texts)):\n        if token_subword_embeddings[original_token_idx]:\n            stacked_embeddings = torch.stack(token_subword_embeddings[original_token_idx])\n            mean_embedding = torch.mean(stacked_embeddings, dim=0)\n            current_sentence_token_features.append(mean_embedding.cpu().numpy())\n        else:\n            current_sentence_token_features.append(np.zeros(model_hf.config.hidden_size))\n\n    features_tokens.extend(current_sentence_token_features)\nUsing device: cuda\nfeatures_tokens = np.array(features_tokens)\nprint(f\"Shape of the token features matrix: {features_tokens.shape}\")\nShape of the token features matrix: (229277, 1024)\n\n\nModel Training\nWe need to train a model for use with the cleanlab library. Initially, the NER labels are transformed using LabelEncoder, which converts the labels into numbers for use in the chosen model.\nIn the actual training of our model, we divide our dataset (features_tokens and labels_ner_codificados) into two parts: one for training (X_treino,y_treino) and another for testing (X_teste, y_teste). We use 25% of the data for the test set and ensure that the proportion of different NER label classes is maintained in both splits, thanks to the stratify parameter. Next, we prepare an array called probabilidades_preditas_teste, which will store the probabilities of each class that our model will assign to the examples in the test set.\nThen, we define and train our classification model. We opted for an SGDClassifier (Stochastic Gradient Descent Classifier). It works by adjusting the parameters of a linear model (in this case, configured to behave like a Logistic Regression using loss=‚Äòlog_loss‚Äô) iteratively, processing one sample at a time, making it fast and scalable. After training the model, we use it to predict the class probabilities for the X_teste set, storing them in probabilidades_preditas_teste. Finally, we also calculate and display the model‚Äôs accuracy on this test set, comparing the predictions with the true y_teste labels.\nA KFold strategy with 5 folds was used to go through the entire dataset separately and independently to obtain out-of-sample predictions for every instance.\nlabel_encoder = LabelEncoder()\nner_labels_encoded = label_encoder.fit_transform(all_labels)\nnum_classes = len(label_encoder.classes_)\n\n\nIssues Found with the Dataset\nNow we delve into the actual application of confident learning techniques. Initially, we use the find_label_issues function from the cleanlab library to identify potentially mislabeled tokens in our NER dataset. We pass the encoded labels (ner_labels_encoded) and the model‚Äôs predicted probabilities (predicted_probabilities) as input.\nskf = StratifiedKFold(n_splits=NUM_FOLDS_CV, shuffle=True, random_state=RANDOM_SEED)\npredicted_probabilities = np.zeros((len(features_tokens), num_classes))\nprint(f\"\\nStarting {NUM_FOLDS_CV}-fold cross-validation\")\n\nfor fold_index, (train_indices, validation_indices) in enumerate(skf.split(features_tokens, ner_labels_encoded)):\n    print(f\"  Processing Fold {fold_index + 1}/{NUM_FOLDS_CV}...\")\n    X_train, X_validation = features_tokens[train_indices], features_tokens[validation_indices]\n    y_train, y_validation = ner_labels_encoded[train_indices], ner_labels_encoded[validation_indices]\n\n    model = SGDClassifier(\n            loss='log_loss',\n            penalty='l2',\n            alpha=0.0001,\n            max_iter=1000,\n            tol=1e-3,\n            random_state=RANDOM_SEED,\n            class_weight='balanced',\n            learning_rate='optimal',\n            early_stopping=True,\n            n_iter_no_change=10,\n            validation_fraction=0.1\n        )\n\n    model.fit(X_train, y_train)\n    print(\"Model trained.\")\n    fold_predicted_probabilities = model.predict_proba(X_validation)\n    predicted_probabilities[validation_indices] = fold_predicted_probabilities\n\n    fold_predictions = model.predict(X_validation)\n    fold_accuracy = accuracy_score(y_validation, fold_predictions)\n    print(f\"    Fold {fold_index + 1} Accuracy: {fold_accuracy:.4f}\")\n\nprint(\"\\nCollection of out-of-sample predicted probabilities finished.\")\nprint(f\"Shape of the predicted probabilities matrix: {predicted_probabilities.shape}\")\nStarting 5-fold cross-validation\n  Processing Fold 1/5...\nModel trained.\n    Fold 1 Accuracy: 0.9680\n  Processing Fold 2/5...\nModel trained.\n    Fold 2 Accuracy: 0.9734\n  Processing Fold 3/5...\nModel trained.\n    Fold 3 Accuracy: 0.9696\n  Processing Fold 4/5...\nModel trained.\n    Fold 4 Accuracy: 0.9720\n  Processing Fold 5/5...\nModel trained.\n    Fold 5 Accuracy: 0.9719\n\nCollection of out-of-sample predicted probabilities finished.\nShape of the predicted probabilities matrix: (229277, 13)\nprint(\"\\nIdentifying labeling issues with cleanlab...\")\n\nlabel_issue_indices = find_label_issues(\n        labels=ner_labels_encoded,\n        pred_probs=predicted_probabilities,\n        return_indices_ranked_by='self_confidence'\n    )\n\nnum_issues_found = len(label_issue_indices)\nprint(f\"Cleanlab identified {num_issues_found} potential labeling issues.\")\npercentage_issues = (num_issues_found / len(all_tokens)) * 100\nprint(f\"This represents {percentage_issues:.2f}% of the total tokens.\")\nIdentifying labeling issues with cleanlab...\nCleanlab identified 2326 potential labeling issues.\nThis represents 1.01% of the total tokens.\nNext, we iterate through the indices of tokens that have potential annotation errors in the NER dataset, comparing the original labels with the model‚Äôs suggested labels. For each token identified as problematic, we retrieve the token and its label, and transform it to its textual form using the label_encoder (method inverse_transform).\nThen, we identify the label predicted by the model with the highest probability and also decode it. We calculate the model‚Äôs confidence in the original label and retrieve the identifier of the sentence to which the token belongs. Finally, we gather all this information into a list of dicts (issues_for_review).\nThe stored dict has the following fields that will be useful for our subsequent analysis:\n\nglobal_token_index: position of the token in our list of all tokens in our dataset\nsentence_id: identifier of the problematic sentence\noriginal_label: the label associated with the token in the dataset\nmodel_suggested_label: the label our model suggests for the token\nmodel_confidence_in_original_label: the probability the model assigns to the original label. Low values mean our model is not very confident that the original label is correct.\nfull_sentence_context: complete sentence where the problematic token was found. It will be used to visualize the problems that will be addressed in a later step.\n\nissues_for_review = []\nfor global_token_index in label_issue_indices:\n    original_token = all_tokens[global_token_index]\n    original_label_encoded = ner_labels_encoded[global_token_index]\n    original_label_str = label_encoder.inverse_transform([original_label_encoded])[0]\n    predicted_label_encoded = np.argmax(predicted_probabilities[global_token_index])\n    predicted_label_str = label_encoder.inverse_transform([predicted_label_encoded])[0]\n\n    confidence_in_original = predicted_probabilities[global_token_index, original_label_encoded]\n\n    sent_id = ids_sentences[global_token_index]\n\n    issues_for_review.append({\n        \"global_token_index\": global_token_index,\n        \"sentence_id\": sent_id,\n        \"token\": original_token,\n        \"original_label\": original_label_str,\n        \"model_suggested_label\": predicted_label_str,\n        \"model_confidence_in_original_label\": confidence_in_original,\n        \"full_sentence_context\": training_sentences[sent_id]\n    })\nWe sort the issues by the lowest model confidence in the originally provided labels and then we visualize the issues found. In the following loop, we have the 20 issues with the lowest model confidence in the original label, i.e., highest distrust.\nsorted_issues_for_review = sorted(issues_for_review, key=lambda x: x['model_confidence_in_original_label'])\n\nfor i, issue in enumerate(sorted_issues_for_review[:min(20, num_issues_found)]):\n    print(f\"\\nProblem #{i+1} (Global Token Index: {issue['global_token_index']})\")\n    print(f\"  Sentence ID: {issue['sentence_id']}\")\n    print(f\"  Token: '{issue['token']}'\")\n    print(f\"  Original Label: {issue['original_label']}\")\n    print(f\"  Model Suggested Label: {issue['model_suggested_label']}\")\n    print(f\"  Model Confidence in Original Label: {issue['model_confidence_in_original_label']:.4f}\")\n\n    sentence_tokens_tags = issue['full_sentence_context']\n\n    first_token_index_in_global_dataset = -1\n    for global_idx, global_sent_id in enumerate(ids_sentences):\n        if global_sent_id == issue['sentence_id']:\n            first_token_index_in_global_dataset = global_idx\n            break\n\n    token_position_in_sentence = issue['global_token_index'] - first_token_index_in_global_dataset\n\n    if not (0 &lt;= token_position_in_sentence &lt; len(sentence_tokens_tags)) or \\\n       sentence_tokens_tags[token_position_in_sentence][0] != issue['token']:\n        found_in_fallback = False\n        for sent_idx, (sent_tk, _) in enumerate(sentence_tokens_tags):\n            if sent_tk == issue['token']:\n                token_position_in_sentence = sent_idx\n                found_in_fallback = True\n                break\n        if not found_in_fallback:\n            print(f\"  WARNING: Could not reliably determine token position for context display for token '{issue['token']}'.\")\n            continue\n\n    context_window = 10\n\n    prev_ctx_start = max(0, token_position_in_sentence - context_window)\n    previous_context_data = sentence_tokens_tags[prev_ctx_start : token_position_in_sentence]\n    formatted_previous_context = [f\"{tk}({tag})\" for tk, tag in previous_context_data]\n\n    problematic_token_text = issue['token']\n    problematic_original_label = issue['original_label']\n    problematic_suggested_label = issue['model_suggested_label']\n    highlighted_token_str = f\"**{problematic_token_text}**(Original:{problematic_original_label}|Suggested:{problematic_suggested_label})**\"\n\n    post_ctx_start = token_position_in_sentence + 1\n    post_ctx_end = min(len(sentence_tokens_tags), post_ctx_start + context_window)\n    subsequent_context_data = sentence_tokens_tags[post_ctx_start : post_ctx_end]\n    formatted_subsequent_context = [f\"{tk}({tag})\" for tk, tag in subsequent_context_data]\n\n    final_context_parts = []\n    if formatted_previous_context:\n        final_context_parts.append(\" \".join(formatted_previous_context))\n\n    final_context_parts.append(highlighted_token_str)\n\n    if formatted_subsequent_context:\n        final_context_parts.append(\" \".join(formatted_subsequent_context))\n\n    print(f\"  Context (¬±{context_window} words): {' '.join(final_context_parts)}\")\n\nprint(\"\\nEnd of issue display.\")\nProblem #1 (Global Token Index: 138519)\n  Sentence ID: 4905\n  Token: 'artigo'\n  Original Label: B-LOCAL\n  Model Suggested Label: B-LEGISLACAO\n  Model Confidence in Original Label: 0.0000\n  Context (¬±10 words): Logo(O) ,(O) tem-se(O) que(O) o(O) **artigo**(Original:B-LOCAL|Suggested:B-LEGISLACAO)** 276(I-LOCAL) do(I-LOCAL) Decreto(I-LOCAL) n¬∫(I-LOCAL) 3.048/99(I-LOCAL) especificamente(O) fixa(O) o(O) dia(O) dois(O)\n\nProblem #2 (Global Token Index: 122878)\n  Sentence ID: 4323\n  Token: 'Autos'\n  Original Label: B-LOCAL\n  Model Suggested Label: B-JURISPRUDENCIA\n  Model Confidence in Original Label: 0.0000\n  Context (¬±10 words): 68(O) 3302-0444/0445(O) ,(O) Rio(B-LOCAL) Branco-AC(I-LOCAL) -(O) Mod(O) .(O) 500258(O) -(O) **Autos**(Original:B-LOCAL|Suggested:B-JURISPRUDENCIA)** n.¬∫(I-LOCAL) 1002199-81.2017.8.01.0000/50000(I-LOCAL) ARA√öJO(O) ,(O) QUARTA(B-ORGANIZACAO) TURMA(I-ORGANIZACAO) ,(O) julgado(O) em(O) 09/05/2017(B-TEMPO)\n\nProblem #3 (Global Token Index: 33548)\n  Sentence ID: 1067\n  Token: ','\n  Original Label: I-LOCAL\n  Model Suggested Label: O\n  Model Confidence in Original Label: 0.0000\n  Context (¬±10 words): CONSTRU√á√ÉO(O) E(O) ALTERA(O) O(O) USO(O) DE(O) LOTES(O) NA(O) QUADRA(B-LOCAL) 1(I-LOCAL) **,**(Original:I-LOCAL|Suggested:O)** DO(I-LOCAL) SETOR(I-LOCAL) DE(I-LOCAL) IND√öSTRIAS(I-LOCAL) GR√ÅFICAS(I-LOCAL) ,(O) NA(O) REGI√ÉO(B-LOCAL) ADMINISTRATIVA(I-LOCAL) DO(I-LOCAL)\n\nProblem #4 (Global Token Index: 122879)\n  Sentence ID: 4323\n  Token: 'n.¬∫'\n  Original Label: I-LOCAL\n  Model Suggested Label: I-JURISPRUDENCIA\n  Model Confidence in Original Label: 0.0000\n  Context (¬±10 words): 3302-0444/0445(O) ,(O) Rio(B-LOCAL) Branco-AC(I-LOCAL) -(O) Mod(O) .(O) 500258(O) -(O) Autos(B-LOCAL) **n.¬∫**(Original:I-LOCAL|Suggested:I-JURISPRUDENCIA)** 1002199-81.2017.8.01.0000/50000(I-LOCAL) ARA√öJO(O) ,(O) QUARTA(B-ORGANIZACAO) TURMA(I-ORGANIZACAO) ,(O) julgado(O) em(O) 09/05/2017(B-TEMPO) ,(O)\n\nProblem #5 (Global Token Index: 56502)\n  Sentence ID: 1863\n  Token: 'TAPE'\n  Original Label: B-LOCAL\n  Model Suggested Label: O\n  Model Confidence in Original Label: 0.0000\n  Context (¬±10 words): COMPROMISSO(O) √â(O) COM(O) A(O) VERDADE(O) POR(O) ISSO(O) O(O) PSDB(B-ORGANIZACAO) 1(O) **TAPE**(Original:B-LOCAL|Suggested:O)** VI(I-LOCAL) APOIA(O) IGOR(B-PESSOA) E(O) TECO(B-PESSOA) .(O)\n\nProblem #6 (Global Token Index: 138520)\n  Sentence ID: 4905\n  Token: '276'\n  Original Label: I-LOCAL\n  Model Suggested Label: I-LEGISLACAO\n  Model Confidence in Original Label: 0.0000\n  Context (¬±10 words): Logo(O) ,(O) tem-se(O) que(O) o(O) artigo(B-LOCAL) **276**(Original:I-LOCAL|Suggested:I-LEGISLACAO)** do(I-LOCAL) Decreto(I-LOCAL) n¬∫(I-LOCAL) 3.048/99(I-LOCAL) especificamente(O) fixa(O) o(O) dia(O) dois(O) do(O)\n\nProblem #7 (Global Token Index: 173708)\n  Sentence ID: 6020\n  Token: 'Penal'\n  Original Label: I-TEMPO\n  Model Suggested Label: I-LEGISLACAO\n  Model Confidence in Original Label: 0.0000\n  Context (¬±10 words): 2.848(I-LEGISLACAO) ,(O) de(O) 7(B-TEMPO) de(I-TEMPO) dezembro(I-TEMPO) de(I-TEMPO) 1940(I-TEMPO) ((O) C√≥digo(B-TEMPO) **Penal**(Original:I-TEMPO|Suggested:I-LEGISLACAO)** )(O) ,(O) passa(O) a(O) vigorar(O) com(O) as(O) seguintes(O) altera√ß√µes(O) :(O)\n\nProblem #8 (Global Token Index: 45419)\n  Sentence ID: 1482\n  Token: '√∫nica'\n  Original Label: I-LOCAL\n  Model Suggested Label: I-ORGANIZACAO\n  Model Confidence in Original Label: 0.0000\n  Context (¬±10 words): fls(O) .(O) 23-TJ(O) pelo(O) douto(O) Juiz(O) de(O) Direito(O) da(O) Vara(B-LOCAL) **√∫nica**(Original:I-LOCAL|Suggested:I-ORGANIZACAO)** da(I-LOCAL) Comarca(I-LOCAL) de(I-LOCAL) Santa(I-LOCAL) Maria(I-LOCAL) do(I-LOCAL) Sua√ßu√≠/MG(I-LOCAL) ,(O) que(O) indeferiu(O)\n\nProblem #9 (Global Token Index: 122880)\n  Sentence ID: 4323\n  Token: '1002199-81.2017.8.01.0000/50000'\n  Original Label: I-LOCAL\n  Model Suggested Label: I-JURISPRUDENCIA\n  Model Confidence in Original Label: 0.0000\n  Context (¬±10 words): ,(O) Rio(B-LOCAL) Branco-AC(I-LOCAL) -(O) Mod(O) .(O) 500258(O) -(O) Autos(B-LOCAL) n.¬∫(I-LOCAL) **1002199-81.2017.8.01.0000/50000**(Original:I-LOCAL|Suggested:I-JURISPRUDENCIA)** ARA√öJO(O) ,(O) QUARTA(B-ORGANIZACAO) TURMA(I-ORGANIZACAO) ,(O) julgado(O) em(O) 09/05/2017(B-TEMPO) ,(O) DJe(O)\n\nProblem #10 (Global Token Index: 28356)\n  Sentence ID: 943\n  Token: 'Estrada'\n  Original Label: I-LOCAL\n  Model Suggested Label: B-LOCAL\n  Model Confidence in Original Label: 0.0000\n  Context (¬±10 words): conformidade(O) com(O) as(O) seguintes(O) especifica√ß√µes(O) :(O) I(O) localiza√ß√£o(O) :(O) DF-001(B-LOCAL) **Estrada**(Original:I-LOCAL|Suggested:B-LOCAL)** Parque(I-LOCAL) Contorno(I-LOCAL) EPCT(I-LOCAL) ,(O) km(O) 12,8(O) ,(O) na(O) Regi√£o(B-LOCAL) Administrativa(I-LOCAL)\n\nProblem #11 (Global Token Index: 57701)\n  Sentence ID: 1912\n  Token: 'Cear√°'\n  Original Label: B-LOCAL\n  Model Suggested Label: I-ORGANIZACAO\n  Model Confidence in Original Label: 0.0000\n  Context (¬±10 words): ementas(O) de(O) julgados(O) dos(O) TREs(O) de(O) Minas(B-LOCAL) Gerais(I-LOCAL) e(O) do(O) **Cear√°**(Original:B-LOCAL|Suggested:I-ORGANIZACAO)** .(O)\n\nProblem #12 (Global Token Index: 54465)\n  Sentence ID: 1795\n  Token: '27/02/2015'\n  Original Label: B-PESSOA\n  Model Suggested Label: B-TEMPO\n  Model Confidence in Original Label: 0.0000\n  Context (¬±10 words): MOURA(I-PESSOA) ,(O) SEXTA(B-ORGANIZACAO) TURMA(I-ORGANIZACAO) ,(O) julgado(O) em(O) 09/12/2014(B-TEMPO) ,(O) DJe(O) **27/02/2015**(Original:B-PESSOA|Suggested:B-TEMPO)** ;(O) HC(B-JURISPRUDENCIA) 312.391/SP(I-JURISPRUDENCIA) ,(O) Rel(O) .(O) Ministro(O) FELIX(B-PESSOA) FISCHER(I-PESSOA) ,(O)\n\nProblem #13 (Global Token Index: 96444)\n  Sentence ID: 3240\n  Token: 'artigo'\n  Original Label: B-ORGANIZACAO\n  Model Suggested Label: B-LEGISLACAO\n  Model Confidence in Original Label: 0.0000\n  Context (¬±10 words): n√£o(O) concretiza√ß√£o(O) ,(O) pelo(O) paciente(O) ,(O) do(O) tipo(O) previsto(O) no(O) **artigo**(Original:B-ORGANIZACAO|Suggested:B-LEGISLACAO)** 187(I-ORGANIZACAO) do(I-ORGANIZACAO) C√≥digo(I-ORGANIZACAO) Penal(I-ORGANIZACAO) Militar(I-ORGANIZACAO) -(O) Crime(O) de(O) Deser√ß√£o(O) -(O)\n\nProblem #14 (Global Token Index: 9642)\n  Sentence ID: 341\n  Token: 'per√≠odo'\n  Original Label: B-TEMPO\n  Model Suggested Label: O\n  Model Confidence in Original Label: 0.0000\n  Context (¬±10 words): $(O) 30.000,00(O) ((O) trinta(O) mil(O) reais(O) )(O) ,(O) alusivos(O) o(O) **per√≠odo**(Original:B-TEMPO|Suggested:O)** de(I-TEMPO) maio(I-TEMPO) a(I-TEMPO) novembro(I-TEMPO) de(I-TEMPO) 2014(I-TEMPO) ;(O) c(O) )(O) R(O)\n\nProblem #15 (Global Token Index: 33464)\n  Sentence ID: 1066\n  Token: 'Federal.Por'\n  Original Label: I-LOCAL\n  Model Suggested Label: O\n  Model Confidence in Original Label: 0.0000\n  Context (¬±10 words): iniciativa(O) do(O) processo(O) legislativo(O) compete(O) privativamente(O) ao(O) Governador(O) do(O) Distrito(B-LOCAL) **Federal.Por**(Original:I-LOCAL|Suggested:O)** isso(O) mesmo(O) ,(O) demonstrado(O) que(O) a(O) iniciativa(O) das(O) leis(O) distritais(O)\n\nProblem #16 (Global Token Index: 84104)\n  Sentence ID: 2801\n  Token: 'Porta-Avi√µes'\n  Original Label: B-LOCAL\n  Model Suggested Label: I-LOCAL\n  Model Confidence in Original Label: 0.0000\n  Context (¬±10 words): ((O) ...(O) )(O) que(O) conheceu(O) o(O) Acusado(O) quando(O) serviu(O) no(O) **Porta-Avi√µes**(Original:B-LOCAL|Suggested:I-LOCAL)** S√£o(I-LOCAL) Paulo(I-LOCAL) ;(O) ((O) ...(O) )(O) que(O) o(O) endere√ßo(O) do(O)\n\nProblem #17 (Global Token Index: 138521)\n  Sentence ID: 4905\n  Token: 'do'\n  Original Label: I-LOCAL\n  Model Suggested Label: I-LEGISLACAO\n  Model Confidence in Original Label: 0.0000\n  Context (¬±10 words): Logo(O) ,(O) tem-se(O) que(O) o(O) artigo(B-LOCAL) 276(I-LOCAL) **do**(Original:I-LOCAL|Suggested:I-LEGISLACAO)** Decreto(I-LOCAL) n¬∫(I-LOCAL) 3.048/99(I-LOCAL) especificamente(O) fixa(O) o(O) dia(O) dois(O) do(O) m√™s(O)\n\nProblem #18 (Global Token Index: 181082)\n  Sentence ID: 6176\n  Token: 'Autom√≥vel'\n  Original Label: I-LOCAL\n  Model Suggested Label: B-LOCAL\n  Model Confidence in Original Label: 0.0000\n  Context (¬±10 words): -(O) S√£o(B-LOCAL) Sebasti√£o(I-LOCAL) ;(O) XXVII(O) -(O) SCIA(B-LOCAL) ((O) Cidade(B-LOCAL) do(I-LOCAL) **Autom√≥vel**(Original:I-LOCAL|Suggested:B-LOCAL)** e(O) Estrutural(B-LOCAL) )(O) ;(O) XXVIII(O) -(O) SIA(B-LOCAL) e(O) Setores(O) Complementares(O)\n\nProblem #19 (Global Token Index: 45418)\n  Sentence ID: 1482\n  Token: 'Vara'\n  Original Label: B-LOCAL\n  Model Suggested Label: I-ORGANIZACAO\n  Model Confidence in Original Label: 0.0000\n  Context (¬±10 words): √†s(O) fls(O) .(O) 23-TJ(O) pelo(O) douto(O) Juiz(O) de(O) Direito(O) da(O) **Vara**(Original:B-LOCAL|Suggested:I-ORGANIZACAO)** √∫nica(I-LOCAL) da(I-LOCAL) Comarca(I-LOCAL) de(I-LOCAL) Santa(I-LOCAL) Maria(I-LOCAL) do(I-LOCAL) Sua√ßu√≠/MG(I-LOCAL) ,(O) que(O)\n\nProblem #20 (Global Token Index: 9667)\n  Sentence ID: 341\n  Token: 'per√≠odo'\n  Original Label: B-TEMPO\n  Model Suggested Label: O\n  Model Confidence in Original Label: 0.0000\n  Context (¬±10 words): e(O) cinco(O) mil(O) reais(O) )(O) por(O) cada(O) m√™s(O) referente(O) ao(O) **per√≠odo**(Original:B-TEMPO|Suggested:O)** de(I-TEMPO) novembro(I-TEMPO) de(I-TEMPO) 2014(I-TEMPO) outubro(I-TEMPO) de(I-TEMPO) 2015(I-TEMPO) ((O) data(O) da(O)\n\nEnd of issue display.\nAt this point, we analyze the output of our confident learning model. We see that in the first identified problems, the model correctly pointed out errors in human annotation. Problems #1 and #2 are clearly examples of erroneously registered legislation: **artigo**(Original:B-LOCAL|Suggested:B-LEGISLACAO)** 276(I-LOCAL) and **Autos**(Original:B-LOCAL|Suggested:B-JURISPRUDENCIA)** n.¬∫(I-LOCAL) 1002199-81.2017.8.01.0000/50000(I-LOCAL).\nHowever, there are examples where our model was confused in pointing out problems in original labels. In problem #3, the comma in the address QUADRA(B-LOCAL) 1(I-LOCAL) ** , **(Original:I-LOCAL|Suggested:O)** DO(I-LOCAL) SETOR(I-LOCAL) DE(I-LOCAL) IND√öSTRIAS(I-LOCAL) GR√ÅFICAS(I-LOCAL) should, in fact, be considered part of the LOCAL label.\nDespite the example of the model‚Äôs mistake, its efficiency in identifying problematic labels is noticeable, attesting to the effectiveness of the applied technique.\n\n\nConclusion\nIn this notebook, we applied Confident Learning techniques using the cleanlab library to detect annotation errors in the LeNER-Br dataset, widely used in Named Entity Recognition (NER) tasks in the Portuguese language.\nWe automatically identified several inconsistent labels between human annotations and the trained model‚Äôs predictions, based on low confidence criteria. It was observed that many of the errors pointed out by the model indeed indicated labeling flaws in the original set, such as the mistaken annotation of legal expressions and jurisprudence names as locations.\nAlthough some false positives were identified ‚Äî such as the case of the comma in the address incorrectly classified by the model ‚Äî the results demonstrate the relevance of the technique for auditing and refining manually annotated datasets.\nWe conclude that the use of Confident Learning represents an effective approach for improving the quality of annotated datasets, especially in sensitive tasks like legal NER, where annotation errors can significantly impact model performance.\nAs a future step, the application of automated or semi-automated retagging techniques is recommended to correct the labels identified as problematic, using the model‚Äôs highest confidence predictions as an initial suggestion for human review."
  },
  {
    "objectID": "posts/extracting-legal-entities-llms-audit-court-decisions.html#using-llms-for-analyzing-decisions-of-the-state-audit-court-of-rio-grande-do-norte",
    "href": "posts/extracting-legal-entities-llms-audit-court-decisions.html#using-llms-for-analyzing-decisions-of-the-state-audit-court-of-rio-grande-do-norte",
    "title": "Using LLMs to Extract Legal Entities from Audit Court Decisions",
    "section": "Using LLMs for Analyzing Decisions of the State Audit Court of Rio Grande do Norte",
    "text": "Using LLMs for Analyzing Decisions of the State Audit Court of Rio Grande do Norte\nAudit Courts in Brazil have a range of constitutional duties, including the judgment of public accounts and the evaluation of personnel acts, such as hiring and retirement of civil servants. The collegiate body, typically composed of seven auditors, holds the authority to impose fines and obligations on public managers who fail to comply with legal norms or operational standards.\nIn the case of the Audit Court of Rio Grande do Norte (TCE/RN), a recent restructuring has established the Procedural Instruction Directorate, responsible for reviewing, processing, and forwarding cases related to external control. Within this directorate, the Decision Control Coordination (CCD) is tasked with registering, monitoring, and tracking the enforcement of the Court‚Äôs decisions and audit recommendations.\nThe CCD also maintains the General Decision Tracking Registry (CGAD), established under Article 431 of the Internal Regulations of TCE/RN, which includes:\n\nGeneral Registry of Fines (CGM), for ongoing tracking of payments made directly to the Court;\nGeneral Registry of Reimbursements (CGD), for tracking orders for restitution to State and Municipal Treasuries;\nGeneral Registry of Recommendations (CGR), for tracking orders to perform or abstain from certain actions;\nGeneral Registry of Management Adjustment Agreements (CGTAG), for agreements negotiated by the Public Ministry with the Court.\n\nThis notebook employs Named Entity Recognition (NER), word embeddings, and Large Language Models (LLMs) to automatically build these registries.\n!pip install pydantic langchain_openai langchain_community gdown langgraph pandas pypdf &gt;&gt; /dev/null\nimport os\nimport getpass\nimport gdown\n\nimport typing\nimport pydantic\n\nimport pandas as pd\n\nfrom langchain.prompts import PromptTemplate, ChatPromptTemplate, FewShotChatMessagePromptTemplate\nfrom langchain_openai import OpenAI, ChatOpenAI\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OPENAI_API_KEY:\")"
  },
  {
    "objectID": "posts/extracting-legal-entities-llms-audit-court-decisions.html#environment-setup",
    "href": "posts/extracting-legal-entities-llms-audit-court-decisions.html#environment-setup",
    "title": "Using LLMs to Extract Legal Entities from Audit Court Decisions",
    "section": "Environment Setup",
    "text": "Environment Setup\nInstall and import the required Python packages for working with LLMs, document parsing, and data manipulation.\nimport pypdf\n\ndef read_pdf(url):\n    return [page.extract_text() for page in pypdf.PdfReader(url).pages]\nurl = \"https://raw.githubusercontent.com/eduardoplima/decisoes-lm/refs/heads/main/dataset.json\"\noutput = \"dataset.json\"\ngdown.download(url, output)\n\nurl = \"https://raw.githubusercontent.com/eduardoplima/decisoes-lm/refs/heads/main/tipos_processos.csv\"\noutput = \"tipos_processos.csv\"\ngdown.download(url, output)\ntipos = pd.read_csv(\"tipos_processos.csv\")\ndf = pd.read_json(\"dataset.json\")\nlen(df)\ndf.columns\n\ndf.head()"
  },
  {
    "objectID": "posts/extracting-legal-entities-llms-audit-court-decisions.html#file-preparation",
    "href": "posts/extracting-legal-entities-llms-audit-court-decisions.html#file-preparation",
    "title": "Using LLMs to Extract Legal Entities from Audit Court Decisions",
    "section": "File Preparation",
    "text": "File Preparation\nDownload the dataset containing decisions from TCE/RN and save it locally for processing.\ndf['texto'] = df['texto'].apply(lambda x: ''.join(x))"
  },
  {
    "objectID": "posts/extracting-legal-entities-llms-audit-court-decisions.html#dataset-loading",
    "href": "posts/extracting-legal-entities-llms-audit-court-decisions.html#dataset-loading",
    "title": "Using LLMs to Extract Legal Entities from Audit Court Decisions",
    "section": "Dataset Loading",
    "text": "Dataset Loading\nLoad the CSV file containing the decisions. We‚Äôll examine the structure and content of the dataset.\nprint(df.iloc[15].texto)"
  },
  {
    "objectID": "posts/extracting-legal-entities-llms-audit-court-decisions.html#initial-exploration",
    "href": "posts/extracting-legal-entities-llms-audit-court-decisions.html#initial-exploration",
    "title": "Using LLMs to Extract Legal Entities from Audit Court Decisions",
    "section": "Initial Exploration",
    "text": "Initial Exploration\nPreview the dataset and review examples from the ‚Äòconclusao‚Äô column, which will be used to extract entities.\nprint(df.iloc[20].texto)"
  },
  {
    "objectID": "posts/extracting-legal-entities-llms-audit-court-decisions.html#manual-rules-for-ner",
    "href": "posts/extracting-legal-entities-llms-audit-court-decisions.html#manual-rules-for-ner",
    "title": "Using LLMs to Extract Legal Entities from Audit Court Decisions",
    "section": "Manual Rules for NER",
    "text": "Manual Rules for NER\nDefine initial heuristics to extract named entities from the decision text using regular expressions.\nprint(df[df.codigo_tipo_processo == 'TAG'].iloc[0].texto)"
  },
  {
    "objectID": "posts/extracting-legal-entities-llms-audit-court-decisions.html#cleaned-entity-extraction",
    "href": "posts/extracting-legal-entities-llms-audit-court-decisions.html#cleaned-entity-extraction",
    "title": "Using LLMs to Extract Legal Entities from Audit Court Decisions",
    "section": "Cleaned Entity Extraction",
    "text": "Cleaned Entity Extraction\nUse regex to extract structured entities such as fines, reimbursements, and obligations from the ‚Äòconclusao‚Äô field.\nllm = ChatOpenAI(temperature=0, model_name='gpt-4o')\nprint(llm)"
  },
  {
    "objectID": "posts/extracting-legal-entities-llms-audit-court-decisions.html#entity-types-fines-and-sanctions",
    "href": "posts/extracting-legal-entities-llms-audit-court-decisions.html#entity-types-fines-and-sanctions",
    "title": "Using LLMs to Extract Legal Entities from Audit Court Decisions",
    "section": "Entity Types: Fines and Sanctions",
    "text": "Entity Types: Fines and Sanctions\nIdentify and extract different types of fines: standard fine, daily coercive fine, percentage-based reimbursement.\npages_relatorio = read_pdf(\"relatorio_teste.pdf\")\npages_relatorio2 = read_pdf(\"relatorio_teste_2.pdf\")\npages_relatorio3 = read_pdf(\"relatorio_teste_3.pdf\")\n\n\nfrom typing import Optional, List\nfrom pydantic import BaseModel, Field\n\nclass Criterio(BaseModel):\n  \"\"\"Crit√©rio para determina√ß√£o de irregularidade\"\"\"\n  descricao: str = Field(description=\"Texto descritivo do crit√©rio\")\n\nclass Encaminhamento(BaseModel):\n  \"\"\"Encaminhamento de irregularidade\"\"\"\n  descricao: str = Field(description=\"Texto descritivo do encaminhamento\")\n\nclass Irregularidade(BaseModel):\n  \"\"\"Irregularidade encontrada no relat√≥rio\"\"\"\n  criterio: Criterio = Field(description=\"Crit√©rio que determina a irregularidade\")\n  encaminhamento: Encaminhamento = Field(description=\"Encaminhamento para a irregularidade\")\n\nclass Relatorio(BaseModel):\n  \"\"\"Relatorio de Auditoria\"\"\"\n  diretoria: str = Field(description=\"Nome da diretoria que realizou a auditoria\")\n  auditores: List[str] = Field(description=\"Auditores que escreveram o relat√≥rio\")\n  irregularidades: List[Irregularidade] = Field(description=\"Irregulidades encontradas no relat√≥rio\")\n\ndef identify_relatorio(relatorio):\n  prompt = PromptTemplate.from_template(\"\"\"\n Voc√™ √© um agente que identifica irregularidades apontadas em relat√≥rios de auditoria.\nVoc√™ recebeu um relat√≥rio de auditoria e precisa identificar quais s√£o as irregularidades apontadas no relat√≥rio.\n\nRelat√≥rio : {input}\n\nSua resposta: \"\"\")\n\n  structured_llm = llm.with_structured_output(schema=Relatorio)\n  chain = prompt | structured_llm\n  response = chain.invoke({\"input\": relatorio})\n  return response\n\ndef identify_pagina_irregularidade(page):\n  prompt = PromptTemplate.from_template(\"\"\"\n Voc√™ √© um agente que identifica se uma p√°gina de um relat√≥rio de auditoria cont√©m uma lista de irregularidades.\nVoc√™ recebeu uma p√°gina de um relat√≥rio de auditoria e precisa identificar se a p√°gina cont√©m irregularidades.\n                                        Responda com Sim apenas se na p√°gina houver uma lista de irregularidades.\n                                        \n                                        Responda apenas com S para sim e N para n√£o.\n\nP√°gina : {input}\n                                        Suas respostas: \"\"\")\n  chain = prompt | llm\n  response = chain.invoke({\"input\": page})\n  return response\n[(i, identify_pagina_irregularidade(p).content) for i,p in enumerate(pages_relatorio2)]\nr = identify_relatorio(''.join(pages_relatorio3))\nfor i in r.irregularidades:\n    print(i)\nr = identify_relatorio(relatorio)\nfor i in r.irregularidade:\n    print(i)\nr2 = identify_relatorio(relatorio2)\nfor i in r2.irregularidade:\n    print(i)\nprompt = PromptTemplate.from_template(\"\"\"\nVoc√™ √© um agente que identifica irregularidades apontadas em relat√≥rios de auditoria.\nVoc√™ recebeu um relat√≥rio de auditoria e precisa identificar quais s√£o as irregularidades apontadas no relat√≥rio.\n\nRelat√≥rio : {input}\n\nSua resposta:\n\"\"\")\n\nchain = prompt | llm\nresponse = chain.invoke({\"input\": relatorio})\nprint(response.content)\nllm\nmonitoramentos = df[df['texto'].str.contains('monitoramento')]\nlen(monitoramentos)\nlen(df)"
  },
  {
    "objectID": "posts/extracting-legal-entities-llms-audit-court-decisions.html#entity-types-recommendations-and-obligations",
    "href": "posts/extracting-legal-entities-llms-audit-court-decisions.html#entity-types-recommendations-and-obligations",
    "title": "Using LLMs to Extract Legal Entities from Audit Court Decisions",
    "section": "Entity Types: Recommendations and Obligations",
    "text": "Entity Types: Recommendations and Obligations\nExtract judicial recommendations and compliance obligations, distinguishing between ‚Äòdo‚Äô and ‚Äòdo not do‚Äô.\ndef classify_decision(state):\n  examples = [\n      {\n          \"input\": \"\"\"\n          - Ado√ß√£o das provid√™ncias cab√≠veis no tocante aos ind√≠cios de impropriedades /irregularidades\n  elencadas na tabela 19 do Relat√≥rio (transcritas acima ), com fundamento no art. 2¬∫, inciso III, da\n  Resolu√ß√£o n¬∫ 012/2023-TCE, notadamente para que a Secretaria de Controle Externo analise a\n  capacidade operacional desta Corte com vistas ao acompanhamento e /ou √† abertura de processo\n  aut√¥nomo, respeitadas a conveni√™ncia e oportunidade, referente aos itens 4.2 (Diverg√™ncia dos\n  dados do planejamento do Projeto, Inconsist√™ncia dos saldos da conta bens m√≥veis entre os\n  registros do SIGEF - Sistema Integrado de Planejamento e Gest√£o Fiscal - e SMI - Sistema de\n  Monitoramento e Informa√ß√µes do Projeto - e defici√™ncias na elabora√ß√£o das notas explicativas ),\n  5.5.1.1 (Invent√°rio f√≠sico ), 5.6.1.1 ( Falhas/defici√™ncias construtivas do Hospital Regional da\n  Mulher Parteira Maria Correia) e 5.6.1.3 (Pend√™ncias de legaliza√ß√£o das obras da Biblioteca\n  C√¢mara Cascudo e da Sede do Servi√ßo Nacional de Emprego ‚Äì SINE).\"\"\",\n          \"output\": \"DETERMINACAO\"}, {\n          \"input\": \"\"\"( a.i) promovam, no prazo de 120 (cento e vinte dias) √∫teis, contados a partir da intima√ß√£o da\n  presente Decis√£o, a apura√ß√£o dos fatos e se verifique a constitucionalidade e legalidade dos\n  v√≠nculos funcionais de cada servidor que figura nos Anexos n¬∫s. 01 e 02 contidos nos Eventos\n  n¬∫s. 04 e 05, al√©m de outros que porventura sejam informados pela DDP em cumprimento ao\n  item b, por meio da instaura√ß√£o de processos administrativos disciplinares individuais ,\n  regulados pela Lei que trata do Estatuto Jur√≠dico dos Servidores do respectivo Munic√≠pio ,\n  com observ√¢ncia dos princ√≠pios do contradit√≥rio, ampla defesa e devido processo legal;\n    ( a.ii) comprovem neste feito, em 05 dias √∫teis ap√≥s ultimado o prazo de defini√ß√£o dos PAD\n  ¬¥s, as conclus√µes de todos os processos administrativos instaurados, no tocante √† elimina√ß√£o\n  de tr√≠plice v√≠nculo funcional identificado e de enquadramento das eventuais acumula√ß√µes\n  d√∫plices nas hip√≥teses permitidas pela Constitui√ß√£o Federal, com a respectiva\n  compatibilidade de hor√°rios, sob pena de, n√£o cumprindo tais obriga√ß√µes nos prazos antes\n  referidos, incidir em multa di√°ria e pessoal ao gestor, no valor de R$ 500,00, com espeque no\n  art. 110 da LCE n¬∫ 464/2012 c/c o art. 326 do RITCE, cabendo √† Diretoria de Despesa com\n  Pessoal monitorar o cumprimento da presente Decis√£o;\"\"\",\n          \"output\": \"DETERMINACAO\"}, {\n          \"input\": \"\"\"Vistos, relatados e discutidos estes autos, em conson√¢ncia ao posicionamento do\n  Corpo t√©cnico e do Minist√©rio P√∫blico de Contas, ACORDAM os Conselheiros, nos termos\n  do voto proposto pelo Conselheiro Relator, julgar a inadmissibilidade da presente den√∫ncia e\n  o seu conseq√ºente arquivamento, com fulcro nos art. 12 do Provimento 002/2020 ‚Äì\n  CORREG/TCE, aprovado pela Resolu√ß√£o 016/2020 ‚Äì TCE e artigo 80, ¬ß 1¬∫, da LOTCE.\n  E ainda, Pela expedi√ß√£o de RECOMENDA√á√ÉO, nos termos do art. 301, III da Resolu√ß√£o\n  009/2012 (RITCE/RN) c/c art. 13, II da Resolu√ß√£o 16/2020 ‚ÄìTCE/RN, ao Executivo\n  Municipal de N√≠sia Floresta /RN, com c√≥pia para o respectivo √≥rg√£o de controle interno, ou\n  setor respons√°vel pelas publica√ß√µes oficiais, lastreada na Constitui√ß√£o Federal de 88/ Art. 37,\n  a fim de que promova e deixe claro os seguintes comportamentos em suas postagens\"\"\",\n          \"output\": \"DETERMINACAO\"},{\n          \"input\": \"\"\"Vistos, relatados e discutidos estes autos, concordando com o proposto pelo Corpo\n  T√©cnico e pelo √≥rg√£o Ministerial de Contas, ACORDAM os Conselheiros, nos termos do voto\n  proferido pelo Conselheiro Relator, julgar pela irregularidade da mat√©ria, nos termos do art .\n  75, inciso I, da Lei Complementar n¬∫ 464/2012, condenando o gestor respons√°vel, Sr. Thiago\n  Meira Mangueira, ao pagamento de multa no valor de R$ 18.774,51 (dezoito mil setecentos e\n  setenta e quatro reais e cinquenta e um centavos ), conforme previsto no art. 21, inciso I ,\n  al√≠nea ‚Äòa‚Äô e ¬ß 1¬∫, da Resolu√ß√£o n¬∫ 012/2016-TCE c /c o art. 107, inciso II, al√≠nea ‚Äúa‚Äù da Lei\n  Complementar n¬∫ 464/2012. \"\"\",\n          \"output\": \"DETERMINACAO\"},\n          {\n              \"input\": \"\"\"Vistos, relatados e discutidos estes autos, acolhendo os fundamentos do parecer\n  ministerial, com substrato no art. 209 V da norma regimental, ACORDAM os Conselheiros ,\n  nos termos do voto proposto pela Conselheira Relatora, julgar pelo ARQUIVAMENTO dos\n  autos.\"\"\",\n              \"output\": \"OUTROS\",\n          },\n          {\n              \"input\": \"\"\"Vistos, relatados e discutidos estes autos, ACORDAM os Conselheiros,  com o\n  impedimento do Conselheiro Presidente Renato Costa Dias, nos termos do voto profposto\n  pela Conselheira Relatora, haja vista os fundamentos f√°tico -jur√≠dicos explanados no excerto\n  antecedente, comprovado documentalmente o adimplemento substancial do plano de\n  redimensionamento/adequa√ß√£o do sistema de ensino natalense, julgar pela EXTIN√á√ÉO do\n  FEITO nos termos do art. 71 da Lei Complementar (estadual) c/c art. 22 ¬ß1¬∞ da LINDB e art .\n  209 V da regra regimental.\"\"\",\n              \"output\": \"OUTROS\",\n          },\n          {\n              \"input\": \"\"\"Vistos, relatados e discutidos estes autos, em conson√¢ncia com o posicionamento da\n  Diretoria de Administra√ß√£o Municipal ‚Äì DAM e do Minist√©rio P√∫blico de Contas ,\n  ACORDAM os Conselheiros, nos termos do voto proferido pelo Conselheiro Relator, julgar\n  pelo reconhecimento da incid√™ncia da Prescri√ß√£o Intercorrente sobre a pretens√£o punitiva e\n  ressarcit√≥ria desta Corte de Contas, nos termos do artigo 111, par√°grafo √∫nico, da Lei\n  Complementar Estadual n¬∫ 464/2012, com o consequente arquivamento dos presentes autos.\n  E ainda, pelo envio de c√≥pia das principais pe√ßas dos autos ao Minist√©rio P√∫blico Estadual ,\n  para conhecimento e atua√ß√£o no √¢mbito de sua compet√™ncia.\"\"\",\n              \"output\": \"OUTROS\",\n          },\n          {\n              \"input\": \"\"\"Vistos, relatados e discutidos estes autos, em disson√¢ncia com o Minist√©rio P√∫blico\n  de Contas, ACORDAM os Conselheiros, nos termos do voto proferido pelo Conselheiro\n  Relator, julgar pela extin√ß√£o do feito, sem julgamento de m√©rito, com o consequente\n  ARQUIV AMENTO dos autos, em virtude da incompatibilidade entre a emiss√£o de parecer\n  pr√©vio pela aprova√ß√£o com ressalvas das contas de governo e a instaura√ß√£o de apura√ß√£o de\n  responsabilidade para aplica√ß√£o de san√ß√µes.\"\"\",\n              \"output\": \"OUTROS\",\n          }\n  ]\n\n  example_prompt = ChatPromptTemplate.from_messages([\n        (\"human\", '''\n         {input} '''),\n        (\"ai\", \"{output}\"),\n    ])\n\n  few_shot_prompt = FewShotChatMessagePromptTemplate(\n    example_prompt=example_prompt,\n    examples=examples,)\n\n  final_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"\"\"Voc√™ √© um classificador de decis√µes de um tribunal de contas.\n         Sua tarefa √© definir se uma decis√£o trata de uma multa ou obriga√ß√£o\n         de fazer.\n\n         Responda com DETERMINACAO se o texto contiver o termo \"multa\" e alguma recomenda√ß√£o ou obriga√ß√£o de fazer\n         Responda com OUTROS se o texto tratar de arquivamento, extin√ß√£o do feito ou prescri√ß√£o da mat√©ria, ou outro assunto que n√£o seja DETERMINACAO\n\n         Responda APENAS com DETERMINACAO ou OUTROS.\"\"\"),\n        few_shot_prompt,\n        (\"human\", \"{input}\"),])\n\n  chain = final_prompt | llm\n  decision_type = chain.invoke({\"input\": state[\"messages\"]}).content\n  state['messages'] = state['messages'] + [decision_type]\n  return state"
  },
  {
    "objectID": "posts/extracting-legal-entities-llms-audit-court-decisions.html#structured-output-example",
    "href": "posts/extracting-legal-entities-llms-audit-court-decisions.html#structured-output-example",
    "title": "Using LLMs to Extract Legal Entities from Audit Court Decisions",
    "section": "Structured Output Example",
    "text": "Structured Output Example\nShow how the extracted entities can be structured for analysis and registration in the CGAD system.\n\nclassify_decision({'messages': [df.iloc[1].texto]})\nprint(df.iloc[1].texto)\nclassify_decision({'messages': [df.iloc[0].texto]})\nprint(df.iloc[0].texto)"
  },
  {
    "objectID": "posts/extracting-legal-entities-llms-audit-court-decisions.html#using-langchain-for-llm-integration",
    "href": "posts/extracting-legal-entities-llms-audit-court-decisions.html#using-langchain-for-llm-integration",
    "title": "Using LLMs to Extract Legal Entities from Audit Court Decisions",
    "section": "Using LangChain for LLM Integration",
    "text": "Using LangChain for LLM Integration\nConfigure a LangChain pipeline using OpenAI or local models to extract and structure named entities.\nfrom typing import Optional, List\n\nfrom pydantic import BaseModel, Field\n\nclass Multa(BaseModel):\n  \"\"\"Determina√ß√£o de multa, com valor e respons√°vel\"\"\"\n  responsavel: str = Field(description=\"Nome do respons√°vel pela multa\")\n  valor: float = Field(description=\"Valor da multa\")\n\nclass Obrigacao(BaseModel):\n  \"\"\"Determina√ß√£o de obriga√ß√£o, com valor e respons√°vel\"\"\"\n  responsavel: str = Field(description=\"Nome do √≥rg√£o ou gestor respons√°vel pela obriga√ß√£o\")\n  descricao: str = Field(description=\"Texto descritivo da obriga√ß√£o\")\n\nclass Decisao(BaseModel):\n  \"\"\"Decis√£o processual do TCE/RN\"\"\"\n  determinacao: List[Optional[Multa | Obrigacao]] = Field(description=\"Determina√ß√£o de multa ou obriga√ß√£o\")\n\ndef identify_decision(state):\n  prompt = PromptTemplate.from_template(\"\"\"\n  Voc√™ √© um agente que identifica listas de determina√ß√µes em textos de decis√µes. Seu objetivo\n  √© extrair um conjunto de textos de uma lista que cont√©m obriga√ß√µes ou imposi√ß√µes de multas. Se n√£o houver\n  determina√ß√µes, responda apenas com \"N/D\"\n\n  Decis√£o : {input}\n\n  Sua resposta:\n  \"\"\")\n\n  structured_llm = llm.with_structured_output(schema=Decisao)\n  chain = prompt | structured_llm\n  response = chain.invoke(state['messages'])\n  state['messages'] = state['messages'] + [response]\n  return state\n\nd = identify_decision({'messages': [df.iloc[1].texto]})\n'determinacao' in d['messages'][-1].__dict__\nd['messages'][-1].determinacao[0]\nprint(df.iloc[1].texto)"
  },
  {
    "objectID": "posts/extracting-legal-entities-llms-audit-court-decisions.html#prompt-engineering",
    "href": "posts/extracting-legal-entities-llms-audit-court-decisions.html#prompt-engineering",
    "title": "Using LLMs to Extract Legal Entities from Audit Court Decisions",
    "section": "Prompt Engineering",
    "text": "Prompt Engineering\nDesign prompts for the LLM to correctly identify and classify the entities mentioned in the text."
  },
  {
    "objectID": "posts/extracting-legal-entities-llms-audit-court-decisions.html#output-validation",
    "href": "posts/extracting-legal-entities-llms-audit-court-decisions.html#output-validation",
    "title": "Using LLMs to Extract Legal Entities from Audit Court Decisions",
    "section": "Output Validation",
    "text": "Output Validation\nValidate the structured output returned by the model against expected schemas.\nfrom langchain_core.tools import StructuredTool\n\ndef search_pessoa(nome):\n  \"\"\"Procura dados de uma pessoa\"\"\"\n  return {\n    'nome': nome,\n    'endere√ßo': 'Av Tal, Bairro Bem Ali, n¬∫ 1',\n    'telefone': '84123456789'\n  }\n\nsearch_pessoa_tool = StructuredTool.from_function(func=search_pessoa, parse_docstring=True, )\nsearch_pessoa_tool.to_json()\nllm_with_tools = llm.bind_tools([search_pessoa_tool])\nresponse = identify_decision({'messages': df.iloc[1].texto})\nresponse['determinacoes'].determinacao[0]\nresponse = llm_with_tools.invoke(str(response['determinacoes'].determinacao[0].responsavel))\nresponse.tool_calls"
  },
  {
    "objectID": "posts/extracting-legal-entities-llms-audit-court-decisions.html#future-improvements",
    "href": "posts/extracting-legal-entities-llms-audit-court-decisions.html#future-improvements",
    "title": "Using LLMs to Extract Legal Entities from Audit Court Decisions",
    "section": "Future Improvements",
    "text": "Future Improvements\nDiscuss how weak supervision or fine-tuned models could improve accuracy and reduce false positives.\ntexto = df.iloc[1].texto\nprint(texto)\nfrom langgraph.prebuilt  import ToolNode\n\ntool_node = ToolNode([search_pessoa_tool])\nimport operator\nfrom typing import Annotated, Sequence, TypedDict\nfrom langchain_core.messages import BaseMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[Sequence[BaseMessage], operator.add]\nfrom langchain_core.messages import ToolMessage\nfrom langgraph.prebuilt import ToolInvocation\n\ndef should_continue_classify(state):\n    if not state['messages'][-1]:\n        return \"end\"\n    else:\n        return \"continue\"\n\ndef should_continue_identify(state):\n    if not state['messages'][-1] == 'DETERMINACAO':\n        return \"end\"\n    else:\n        return \"continue\"\n\ndef call_model(state):\n    messages = state[\"messages\"]\n    response = llm_with_tools.invoke(messages)\n    return response\n\ndef call_tool(state):\n  print(state['messages'][-1].__dict__.keys())\n  if 'determinacao' in state['messages'][-1].__dict__.keys():\n    for determinacao in state['messages'][-1].determinacao:\n      if not determinacao.responsavel:\n        continue\n      response = llm_with_tools.invoke(str(determinacao.responsavel))\n      print(response)\n      response = tool_node.invoke({'messages': [response]})\n      state['messages'] = state['messages'] + [response['messages']]\n      return state\nfrom langgraph.graph import END, StateGraph, START\n\nworkflow = StateGraph(AgentState)\nworkflow.add_node(\"classify\", classify_decision)\nworkflow.add_node(\"identify\", identify_decision)\nworkflow.add_node(\"search\", call_tool)\nworkflow.add_edge(START, \"classify\")\nworkflow.add_edge(\"search\", END)\n\nworkflow.add_conditional_edges(\n    \"classify\",\n    should_continue_classify,\n    {\n        \"continue\": \"identify\",\n        \"end\": END,\n    },\n)\n\nworkflow.add_conditional_edges(\n    \"identify\",\n    should_continue_classify,\n    {\n        \"continue\": \"search\",\n        \"end\": END,\n    },\n)\n\napp = workflow.compile()\nfrom IPython.display import Image, display\nfrom langchain_core.runnables.graph import CurveStyle\n\ntry:\n    graph_image = app.get_graph(xray=True).draw_mermaid_png(curve_style=CurveStyle.NATURAL)\n    display(Image(graph_image))\nexcept Exception as e:\n    print(e)\ninputs = {\"messages\": [df.iloc[1].texto]}\nresult = app.invoke(inputs)\nresult"
  },
  {
    "objectID": "posts/extracting-legal-entities-llms-audit-court-decisions.html#conclusion",
    "href": "posts/extracting-legal-entities-llms-audit-court-decisions.html#conclusion",
    "title": "Using LLMs to Extract Legal Entities from Audit Court Decisions",
    "section": "Conclusion",
    "text": "Conclusion\nThis notebook outlines a hybrid approach using heuristics and LLMs to extract key data from legal decisions, contributing to improved compliance tracking."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I‚Äôm a Portuguese-Brazilian data scientist, Python developer, and public sector auditor currently based in Natal/RN with over 15 years of experience working at the Audit Court of Rio Grande do Norte (TCE/RN). I‚Äôve spent most of my career contributing to public expenditure audits as an IT expert and data analyst‚Äîdeveloping tools, uncovering patterns in large datasets, and applying AI to detect risks and inefficiencies in government spending.\nRight now, I‚Äôm diving deep into NLP, transformers, semi-supervised pipelines, and exploring how LLMs can help extract insights from complex legal decisions. I‚Äôm also finishing a Master‚Äôs in Artificial Intelligence.\n\n\n\nPython, Pandas, scikit-learn, Hugging Face Transformers\n\nAirflow, SQL, Postgres, Cleanlab, LangChain\n\nMachine Learning, Weak Supervision, Legal NLP\n\n\n\n\nWhen I‚Äôm not coding or reading about AI, you‚Äôll probably find me:\n\nüèÑ Surfing or at the beach\n\n‚ôüÔ∏è Playing rapid games on chess.com/member/lima85\n\nüç£ Hunting for good Japanese food\n\n‚úàÔ∏è Traveling to discover new places and cultures\n\n\n\n\n\nüíº LinkedIn: linkedin.com/in/eduardo-p-lima/\n\nüêô GitHub: github.com/eduardoplima"
  },
  {
    "objectID": "about.html#hey-im-eduardo",
    "href": "about.html#hey-im-eduardo",
    "title": "About Me",
    "section": "",
    "text": "I‚Äôm a Portuguese-Brazilian data scientist, Python developer, and public sector auditor currently based in Natal/RN with over 15 years of experience working at the Audit Court of Rio Grande do Norte (TCE/RN). I‚Äôve spent most of my career contributing to public expenditure audits as an IT expert and data analyst‚Äîdeveloping tools, uncovering patterns in large datasets, and applying AI to detect risks and inefficiencies in government spending.\nRight now, I‚Äôm diving deep into NLP, transformers, semi-supervised pipelines, and exploring how LLMs can help extract insights from complex legal decisions. I‚Äôm also finishing a Master‚Äôs in Artificial Intelligence.\n\n\n\nPython, Pandas, scikit-learn, Hugging Face Transformers\n\nAirflow, SQL, Postgres, Cleanlab, LangChain\n\nMachine Learning, Weak Supervision, Legal NLP\n\n\n\n\nWhen I‚Äôm not coding or reading about AI, you‚Äôll probably find me:\n\nüèÑ Surfing or at the beach\n\n‚ôüÔ∏è Playing rapid games on chess.com/member/lima85\n\nüç£ Hunting for good Japanese food\n\n‚úàÔ∏è Traveling to discover new places and cultures\n\n\n\n\n\nüíº LinkedIn: linkedin.com/in/eduardo-p-lima/\n\nüêô GitHub: github.com/eduardoplima"
  }
]